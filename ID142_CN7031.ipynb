{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherinjames-sj/GoogleColab_BigData/blob/main/ID142_CN7031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx9-Fre4FMda"
      },
      "source": [
        "# **Big Data Analytics [CN7031] CRWK 2022-23**\n",
        "1.   Sherin James U2298965\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdMZR-9QTwG3"
      },
      "source": [
        "\n",
        "# **Initiate and Configure Spark**\n",
        "\n",
        "---\n",
        "PySpark is installed and the configuration of PySpark is prepared prior to its start-up.\n",
        "\n",
        "In Spark, the SparkSession class is the entry point to all functions. SparkSession is also the entry point to Spark SQL, one of the very first objects that a Spark developer creates when developing a Spark SQL application. A Spark developer creates a SparkSession using the SparkSession class. As you use the builder method (that provides access to the Builder API, which you can use to configure the session), you will have access to the builder method (which gives you access to the Builder API).\n",
        "\n",
        "To create an initial SparkSession, you will need to type the following commands.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiqzQti9-b-f",
        "outputId": "1619a0de-7fb8-484b-b5e2-c5213d555655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=3b86348f443a9f87e2aa845259fc50c696b6c79ec3af7982fa2b5d821fb0c349\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "# PySpark is installed and the configuration of PySpark is prepared prior to its start-up\n",
        "\n",
        "!pip3 install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7vmYMqYN68L"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Group id: 142\") \\\n",
        "    .config(\"spark.some.config.option\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2CZVl6TOQX"
      },
      "source": [
        "\n",
        "# **Load Data**\n",
        "\n",
        "---\n",
        "Next, the Google Drive will be connected to Google Colaboratory and then the preferred dataset will be loaded. The attached folder contains all of the CSE-CIC-IDS2018 dataset which was created by the University of New Brunswick in order to analyze DDoS data. As you can see here we are attaching the entire folder of the dataset which consists of CSE-CIC-IDS2018. \n",
        "It is of utmost importance that we view the dataset within the collaborative environment. The cross reference process begins with loading the file correctly, ensuring that all the data is presented correctly, but also checking what the data is if we are loading it for the first time. By using this method, we find out what type of data the data is, how many columns and rows there are, how many entries there are, and what characteristics there are in the whole dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-zSlLkNtCCw"
      },
      "outputs": [],
      "source": [
        "#mount your Google Drive:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BbILXmUvaqg"
      },
      "outputs": [],
      "source": [
        "#Load the entire Dataset folder from google drive in colab:\n",
        "\n",
        "groupid142_df = spark.read.load(\"/content/drive/MyDrive/DATASET\", format=\"csv\", inferSchema=True,  header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGumwGzn9_7y"
      },
      "outputs": [],
      "source": [
        "#Viewing the datafile:\n",
        "groupid142_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOFR7gldnMGg"
      },
      "outputs": [],
      "source": [
        "# Using printschema we recieve break down of the data along with the names of the columns and formats of the data, is presented\n",
        "\n",
        "groupid142_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWfjrvdlupqq"
      },
      "outputs": [],
      "source": [
        "# As we move forward in this section, we will find out what types of data we will be working with\n",
        "groupid142_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vx2j88qu03n"
      },
      "outputs": [],
      "source": [
        "#the count of total rows using the count method over data frame\n",
        "groupid142_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiTMgCppv-70"
      },
      "outputs": [],
      "source": [
        "len (groupid142_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxSauI8J2QrD"
      },
      "outputs": [],
      "source": [
        "groupid142_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8CXZwl1ob9G"
      },
      "outputs": [],
      "source": [
        "groupid142_df.select('Label').groupBy('Label').count().orderBy('count', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RjT7_UHAqic"
      },
      "source": [
        "\n",
        "# **Task 1: Spark SQL [30 marks]**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Spark SQL module enables Hadoop Hive queries to be run up to 100 times faster on existing deployments and data using a programming abstraction called DataFrames. This is thanks to this software module. The Spark SQL module allows Hadoop Hive queries to run at up to 100 times faster on existing data. When it comes to performance, it can be viewed as being very similar to a distributed SQL query engine in the sense that it can perform in the same manner. As an additional feature, it also provides a type of abstraction which is known as DataFrames, which is similar in some ways to the SQL query engine in a database in terms of its functionality.\n",
        "\n",
        "The following operations were used in this query:\n",
        "\n",
        "SELECT: Returns a set of records as a result of its statement;\n",
        "DISTINCT: It only returns distinct values (different);\n",
        "FROM: Data values can be retrieved from a table or view;\n",
        "WHERE: Records can be filtered with it;\n",
        "GROUP BY: Creates summary rows based on rows with the same values;\n",
        "LIMIT: Indicates how many records should be returned.\n",
        "\n",
        "The aggregate functions which I used are:\n",
        "\n",
        "COUNT: Returns how many rows there are in total;\n",
        "MAX: Returns the highest value;\n",
        "SUM: Returns the value calculated by this function;\n",
        "AVG: The average of the values is returned by this function.\n"
      ],
      "metadata": {
        "id": "mMwscdO-Dixv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fCFTcOQBLD2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4zB_WWORH_p"
      },
      "outputs": [],
      "source": [
        "# The query below retrieves values from the view that I've created which consists of distinct values from DST PORT. \n",
        "# I have aggregated the columns `Subflow Fwd Byts`, `Tot Fwd Pkts`, `Flow Duration`, `Subflow Fwd Byts` and \n",
        "# then grouped them by DST PORT where the value of \n",
        "# the LABEL column was BENIGN and the data displayed in the view were from between a certain Timestamp. \n",
        "\n",
        "groupid142_df.createOrReplaceTempView(\"Subflow\")\n",
        "\n",
        "print(\"\\n============================================= SUBFLOW =============================================\\n\")\n",
        "\n",
        "sqlsparkDF= spark.sql(\"\"\"SELECT Distinct `Dst Port` AS DESTINATION_PORT, \n",
        "count(`Subflow Fwd Byts`) AS COUNT_SUBFLOW_FWD_BYTS, max(`Tot Fwd Pkts`) AS MAX_TOTAL_FWD_PKTS, sum(`Flow Duration`) AS SUM_FLOW_DURATION, \n",
        "avg(`Subflow Fwd Byts`) AS AVG_SUBFLOW_FWD_BYTS FROM Subflow Group By `Dst Port`  Limit 10 \"\"\") \n",
        "\n",
        "sqlsparkDF.show()\n",
        "\n",
        "# pandasDF = sqlsparkDF.toPandas()\n",
        "# print(\"\\n==================== BARH GRAPH ====================\\n\")\n",
        "# pandasDF.plot(x = 'DESTINATION_PORT', y = 'MAX_TOTAL_FWD_PKTS', kind = 'barh')\n",
        "# plt.xlabel(\"DESTINATION_PORT\")\n",
        "# plt.ylabel(\"MAX_TOTAL_FWD_PKTS\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px6ndUu-SaO1"
      },
      "outputs": [],
      "source": [
        "# In the query below I have fetched for values from the view which I have created which consists of the values from Protocol, and \n",
        "# performed the aggregate functions on the column `Dst Port', `Fwd Pkts/s`, `Bwd Pkts/s`, `Fwd Header Len` and also performed the mathematical functions of ceiling and round and \n",
        "# then grouped them by Protocol where the value of the LABEL column was BENIGN.\n",
        "# I have used 2 types of visualization method to plot my output and they are BAR graph and PIE graph.\n",
        "\n",
        "groupid142_df.createOrReplaceTempView(\"PROTOCOL_LABEL\")\n",
        "\n",
        "sqlsparkDF= spark.sql(\"\"\"SELECT Count('Dst Port') as COUNT_DESTINATION_PORT, Protocol as PROTOCOL, round(max(`Fwd Pkts/s`)) as ROUND_FWD_PKTS, \n",
        "ceiling(avg(`Bwd Pkts/s`)) as CEILING_BWD_PKTS, sum(`Fwd Header Len`) as SUM_FWD_HEADER_LEN from PROTOCOL_LABEL where Label='Benign' GROUP BY Protocol\"\"\")\n",
        "\n",
        "# sqlsparkDF.show()\n",
        "\n",
        "pandasDF = sqlsparkDF.toPandas()\n",
        "print(\"\\n==================== PIE GRAPH ====================\\n\")\n",
        "pandasDF.plot(x = 'PROTOCOL', y = 'CEILING_BWD_PKTS', labels=['Protocol_6', 'Protocol_17', 'Protocol_0'], kind = 'pie')\n",
        "#x is the color in the pie chart, i.e. PROTOCOL which is 6(y=4251), 17(y=1146), 0(y=145)\n",
        "plt.legend(['4251_BWD_PKTS', '1146_BWD_PKTS', '145_BWD_PKTS'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJhGbI2BKpx"
      },
      "source": [
        "\n",
        "# **Task 2 - Part1: PySpark [45 marks]**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3eiN9geBQRf"
      },
      "outputs": [],
      "source": [
        "# Analytical method 1: DESCRIPTIVE METHOD\n",
        "#Here, the description of the whole dataset is displayed and represented visually as well.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "datatrain = pd.read_csv('/content/drive/MyDrive/DATASET/02-14-2018.csv')\n",
        "datatest = pd.read_csv('/content/drive/MyDrive/DATASET/02-14-2018.csv')\n",
        "datatrain['source'] = 'train'\n",
        "datatest['source'] = 'test'\n",
        "\n",
        "# Concatinating both of the train and test data.\n",
        "dataconcat = pd.concat([datatrain, datatest], ignore_index=True)\n",
        "print(dataconcat)\n",
        "\n",
        "# Test&Train Data\n",
        "allcolumns = dataconcat.columns\n",
        "\n",
        "# store numerical and categorical column in two different variables. It comes handy during visualizaion.\n",
        "column_num = dataconcat._get_numeric_data().columns\n",
        "column_cat = list(set(allcolumns)-set(column_num))\n",
        "\n",
        "# below we are plotting the describe() function\n",
        "# Pandas describe() function displays mean, stddev, percentile, count, and IQR values of the dataframe.\n",
        "# By default describe() function only takes Int and float data type variables into consideration for calculations.\n",
        "\n",
        "dataconcat.describe()\n",
        "\n",
        "groupid142_describe = dataconcat.describe(include=['int64','float64'])\n",
        "groupid142_describe.reset_index(inplace=True)\n",
        "\n",
        "# To remove any variable from plot\n",
        "groupid142_describe = groupid142_describe[groupid142_describe['index'] != 'count']\n",
        "\n",
        "for i in column_num:\n",
        "  if i in ['index']:\n",
        "    continue\n",
        "  sns.factorplot(x=\"index\", y=i, data=groupid142_describe)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ_-hgdeiMle"
      },
      "outputs": [],
      "source": [
        "# Analytical method 2: CORRELATION METHOD\n",
        "\n",
        "''' \n",
        "Correlation analysis is used to figure out if there exists a relationship between variables.\n",
        "Correlation helps us to study both the strength and direction of the relationship between two sets of variables.\n",
        "''' \n",
        "\n",
        "print(\"Correlation between the columns Dst Port and Bwd Pkt Len Std is : \")\n",
        "result = groupid142_df.stat.corr(\"Dst Port\", \"Bwd Pkt Len Std\")\n",
        "print(result)\n",
        "\n",
        "import seaborn as sns\n",
        "# checking correlation using heatmap\n",
        "#Loading dataset\n",
        "flights = pd.read_csv('/content/drive/MyDrive/DATASET/02-14-2018.csv')\n",
        "#plotting the heatmap for correlation\n",
        "ax = plt.subplots(figsize=(10,9))\n",
        "ax = sns.heatmap(flights.corr(), annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JGQHXYliMK5"
      },
      "outputs": [],
      "source": [
        "# Analytical method 3: chisquare test gives us the pvalues and test satistics.\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "VectAssem = VectorAssembler(\n",
        "inputCols=[\"Fwd Act Data Pkts\", \"Fwd Seg Size Min\"],\n",
        "outputCol= \"valuesofvector\")\n",
        "VectAssem\n",
        "vect_DF = VectAssem.transform(groupid142_df).select(\"Fwd Act Data Pkts\",\"valuesofvector\")\n",
        "vect_DF.show()\n",
        "\n",
        "results = ChiSquareTest.test(vect_DF, \"valuesofvector\", \"Fwd Act Data Pkts\").head()\n",
        "\n",
        "print(\"p-value = \" + str (results.pValues))\n",
        "print(\"test statistics =\" + str (results.statistics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHft1Jht1Qxl"
      },
      "source": [
        "\n",
        "# **Task 2 - Part2: PySpark [15 marks]**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**\n",
        "\n",
        " A Linear regression model is a statistical technique that is used to predict the value of one variable by comparing it with the value of another. An independent variable refers to the variable that you wish to predict, while a dependent variable refers to the variable you wish to use to forecast another variable.\n",
        "\n",
        "With the use of linear regression, it is possible to determine the nature and strength of the relationship between a series of independent variables and a dependent variable. In order to develop models and make predictions, such as predicting the stock price of a company, this information can be used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8neaCri4CQ74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcZfiIcq1Qxn"
      },
      "outputs": [],
      "source": [
        "# Machine Learning Technique: Linear Regression\n",
        "\n",
        "from pandas import read_csv\n",
        "#to import linear regression class\n",
        "from sklearn.linear_model import LinearRegression\n",
        "#to split data into training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "RANDOM_STATE_SEED = 12\n",
        "data = read_csv(\"/content/drive/MyDrive/DATASET/02-14-2018.csv\",  usecols=[0, 1, 3, 4, 5, 79])\n",
        "\n",
        "#data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "data.replace(to_replace=[\"FTP-BruteForce\", \"SSH-Bruteforce\"], value=\"Malicious\", inplace=True)\n",
        "\n",
        "data[\"Label\"].value_counts()\n",
        "\n",
        "data1 = data[data[\"Label\"] == \"Benign\"][:380943]\n",
        "data2 = data[data[\"Label\"] == \"Malicious\"][:380943]\n",
        "data_equal = pd.concat([ data1,data2], axis =0)\n",
        "\n",
        "data_equal.replace(to_replace=\"Benign\", value=0, inplace=True)\n",
        "data_equal.replace(to_replace=\"Malicious\", value=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hfWiyydjX5O"
      },
      "outputs": [],
      "source": [
        "data_equal.info()\n",
        "train, test = train_test_split(data_equal, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vlMvsbcj8OA"
      },
      "outputs": [],
      "source": [
        "print(\"Full dataset:\")\n",
        "print(\"Benign: \" + str(data_equal[\"Label\"].value_counts()[[0]].sum()))\n",
        "print(\"Malicious: \" + str(data_equal[\"Label\"].value_counts()[[1]].sum()))\n",
        "print(\"\\n---------------\")\n",
        "\n",
        "print(\"Training set:\")\n",
        "print(\"Benign: \" + str(train[\"Label\"].value_counts()[[0]].sum()))\n",
        "print(\"Malicious: \" + str(train[\"Label\"].value_counts()[[1]].sum()))\n",
        "print(\"\\n---------------\")\n",
        "\n",
        "print(\"Test set:\")\n",
        "print(\"Benign: \" + str(test[\"Label\"].value_counts()[[0]].sum()))\n",
        "print(\"Malicious: \" + str(test[\"Label\"].value_counts()[[1]].sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFoqO84redAX"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import random\n",
        "#idex locator method of the df. This is the y. Our target variables.\n",
        "labelpred = data_equal.iloc[:, 5]\n",
        "\n",
        "#print(labelpred)\n",
        "\n",
        "#Our input variables.\n",
        "rem_col = data_equal.iloc[:, 0:4]\n",
        "\n",
        "#print(rem_col)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(rem_col, labelpred, random_state=0)\n",
        "data['Label'].value_counts(normalize = True)\n",
        "\n",
        "\n",
        "#create model\n",
        "\n",
        "linreg = LinearRegression()\n",
        "\n",
        "linreg.fit(x_train, y_train)\n",
        "\n",
        "predictions = linreg.predict(x_test)\n",
        "\n",
        "print(predictions, \"\\n\")\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU59l3Pwvuku"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, predictions.round())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Lx9-Fre4FMda"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}